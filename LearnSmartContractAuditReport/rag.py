# -*- coding: utf-8 -*-
import os
import argparse
from pypdf import PdfReader  # Th√™m d√≤ng import n√†y
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
import ollama

# C·∫•u h√¨nh
EMBEDDING_MODEL = "all-mpnet-base-v2"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
VECTOR_DB_PATH = "vector_store"

def init_vector_db():
    """Kh·ªüi t·∫°o vector database n·∫øu ch∆∞a t·ªìn t·∫°i"""
    if not os.path.exists(VECTOR_DB_PATH):
        os.makedirs(VECTOR_DB_PATH)
        # T·∫°o m·ªôt vector store r·ªóng
        return FAISS.from_texts([""], HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL))
    
    # Ki·ªÉm tra xem file index.faiss c√≥ t·ªìn t·∫°i kh√¥ng
    if not os.path.exists(os.path.join(VECTOR_DB_PATH, "index.faiss")):
        return FAISS.from_texts([""], HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL))
    
    # T·∫£i vector store t·ª´ th∆∞ m·ª•c
    return FAISS.load_local(
        VECTOR_DB_PATH,
        HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL),
        allow_dangerous_deserialization=True
    )

def process_pdf(pdf_path):
    """X·ª≠ l√Ω file PDF v√† tr·∫£ v·ªÅ c√°c chunks"""
    text = "\n".join([page.extract_text() for page in PdfReader(pdf_path).pages if page.extract_text()])
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    return splitter.split_text(text)

def learn_command(pdf_path):
    """X·ª≠ l√Ω l·ªánh h·ªçc"""
    try:
        # X·ª≠ l√Ω file/th∆∞ m·ª•c PDF
        if os.path.isdir(pdf_path):
            for file in os.listdir(pdf_path):
                if file.endswith(".pdf"):
                    chunks = process_pdf(os.path.join(pdf_path, file))
        else:
            chunks = process_pdf(pdf_path)
        
        # C·∫≠p nh·∫≠t vector store
        vector_store = init_vector_db()
        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        new_db = FAISS.from_texts(chunks, embeddings)
        vector_store.merge_from(new_db)
        vector_store.save_local(VECTOR_DB_PATH)
        
        # T·∫°o b√°o c√°o h·ªçc t·∫≠p
        report_prompt = f"""
        H√£y t√≥m t·∫Øt nh·ªØng ki·∫øn th·ª©c ƒë√£ h·ªçc ƒë∆∞·ª£c t·ª´ c√°c t√†i li·ªáu sau:
        {chunks[:3]}... (tr√≠ch d·∫´n 3 ƒëo·∫°n ƒë·∫ßu)
        """
        response = ollama.generate(model="codellama", prompt=report_prompt)
        print("\nüìö B√°o c√°o h·ªçc t·∫≠p:")
        print(response["response"])
        
    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")

def query_command():
    """Giao di·ªán chat h·ªèi ƒë√°p"""
    vector_store = init_vector_db()
    print("\nüí¨ Ch·∫ø ƒë·ªô h·ªèi ƒë√°p (Nh·∫≠p 'exit' ƒë·ªÉ tho√°t)")
    
    while True:
        query = input("\nü§î C√¢u h·ªèi: ")
        if query.lower() == "exit":
            break
        
        # Truy xu·∫•t th√¥ng tin
        docs = vector_store.similarity_search(query, k=3)
        context = "\n".join([d.page_content for d in docs])
        
        # T·∫°o prompt
        prompt = f"""
        [VAI TR√í] B·∫°n l√† chuy√™n gia b·∫£o m·∫≠t smart contract v·ªõi ki·∫øn th·ª©c t·ª´ c√°c audit report.
        [Y√äU C·∫¶U] Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh v√† ki·∫øn th·ª©c chuy√™n m√¥n.
        [NG·ªÆ C·∫¢NH] {context}
        [C√ÇU H·ªéI] {query}
        [TR·∫¢ L·ªúI] H√£y tr√¨nh b√†y chi ti·∫øt g·ªìm:
        - Ph√¢n t√≠ch v·∫•n ƒë·ªÅ
        - V√≠ d·ª• minh h·ªça (n·∫øu c√≥)
        - Khuy·∫øn ngh·ªã gi·∫£i ph√°p
        - Tham chi·∫øu audit report li√™n quan
        """
        
        # G·ªçi LLM
        response = ollama.generate(model="codellama", prompt=prompt)
        print("\nüí° Tr·∫£ l·ªùi:")
        print(response["response"])
        
        # Sinh test cases cho fuzzing
        fuzz_prompt = f"""
        D·ª±a tr√™n c√¢u tr·∫£ l·ªùi tr∆∞·ªõc, h√£y ƒë·ªÅ xu·∫•t 3 test case cho c√¥ng c·ª• fuzzing:
        {response["response"]}
        """
        fuzz_response = ollama.generate(model="codellama", prompt=fuzz_prompt)
        print("\nüîß ƒê·ªÅ xu·∫•t test cases cho fuzzing:")
        print(fuzz_response["response"])

def main():
    parser = argparse.ArgumentParser(description="Smart Contract Audit Expert System")
    subparsers = parser.add_subparsers(dest='command')
    
    # L·ªánh h·ªçc
    learn_parser = subparsers.add_parser('learn', help='H·ªçc t·ª´ file/th∆∞ m·ª•c PDF')
    learn_parser.add_argument('path', help='ƒê∆∞·ªùng d·∫´n file/th∆∞ m·ª•c PDF')
    
    # L·ªánh h·ªèi
    subparsers.add_parser('query', help='Ch·∫ø ƒë·ªô h·ªèi ƒë√°p')
    
    args = parser.parse_args()
    
    if args.command == "learn":
        learn_command(args.path)
    elif args.command == "query":
        query_command()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()